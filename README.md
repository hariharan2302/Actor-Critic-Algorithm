# ğŸ® Actor-Critic Reinforcement Learning Implementation

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.0+-red.svg)](https://pytorch.org)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.28+-green.svg)](https://gymnasium.farama.org)

A comprehensive implementation of the **Actor-Critic (A2C)** algorithm applied to multiple reinforcement learning environments, from simple grid worlds to complex continuous control and image-based tasks.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Environments](#environments)
- [Architecture](#architecture)
- [Implementation Details](#implementation-details)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [File Structure](#file-structure)
- [Key Features](#key-features)

## ğŸ¯ Overview

This project implements the **Advantage Actor-Critic (A2C)** algorithm, a policy gradient method that combines the benefits of both value-based and policy-based reinforcement learning approaches. The implementation demonstrates the versatility of A2C across various environment types:

- **Discrete environments**: GridWorld, CartPole
- **Continuous control**: MountainCar, BipedalWalker  
- **Image-based environments**: CarRacing

### ğŸ§  What is Actor-Critic?

The Actor-Critic method consists of two neural networks:
- **Actor (Policy Network)**: Learns the optimal policy Ï€(a|s) - what action to take in each state
- **Critic (Value Network)**: Learns the value function V(s) - estimates the expected return from each state

The critic provides feedback to the actor, reducing variance in policy gradient estimates and improving learning stability.

## ğŸŒ Environments

### 1. ğŸ² Custom GridWorld Environment
- **State Space**: 4Ã—4 grid (16 discrete states)
- **Action Space**: 4 discrete actions (up, down, left, right)
- **Objective**: Navigate from start (0,0) to goal (3,3) while avoiding obstacles
- **Rewards**: Goal (+10), Obstacles (-1, -2, -5)

### 2. ğŸª CartPole-v1
- **State Space**: 4 continuous variables (position, velocity, angle, angular velocity)
- **Action Space**: 2 discrete actions (left, right)
- **Objective**: Balance pole on cart for 500 timesteps
- **Success Criteria**: Average reward â‰¥ 500

### 3. ğŸ”ï¸ MountainCarContinuous-v0
- **State Space**: 2 continuous variables (position, velocity)
- **Action Space**: 1 continuous action (force applied)
- **Objective**: Drive car up the mountain to reach the flag
- **Challenge**: Sparse rewards, requires momentum building

### 4. ğŸš¶ BipedalWalker-v3
- **State Space**: 24 continuous variables (hull angle, angular velocity, leg positions, etc.)
- **Action Space**: 4 continuous actions (hip/knee joint torques)
- **Objective**: Walk forward as far as possible without falling
- **Complexity**: High-dimensional continuous control

### 5. ğŸï¸ CarRacing-v2
- **State Space**: 96Ã—96Ã—3 RGB images
- **Action Space**: 3 continuous actions (steering, gas, brake)
- **Objective**: Complete racing track as quickly as possible
- **Challenge**: Vision-based control with CNN feature extraction

## ğŸ—ï¸ Architecture

### Standard A2C Agent (Discrete/Continuous)
```
Input Layer â†’ Hidden Layer (16 units) â†’ Output Layer
                    â†“
Actor Network: State â†’ Action Probabilities/Values
Critic Network: State â†’ State Value
```

### Image-based A2C Agent (CarRacing)
```
CNN Feature Extractor:
96Ã—96Ã—3 â†’ Conv2D â†’ Conv2D â†’ Flatten â†’ FC Layers

Actor: Features â†’ Action Distribution
Critic: Features â†’ State Value
```

## ğŸ”§ Implementation Details

### Key Components

1. **Neural Network Architecture**
   - Fully connected networks with ReLU activation
   - Separate networks for actor and critic
   - CNN-based feature extraction for image inputs

2. **Training Algorithm**
   - Policy gradient with baseline (advantage estimation)
   - Discounted reward calculation
   - Adam optimizer for both networks
   - MSE loss for critic, policy gradient loss for actor

3. **Hyperparameters**
   | Environment | Episodes | Learning Rate | Discount Factor |
   |-------------|----------|---------------|-----------------|
   | GridWorld   | 1000     | 0.001         | 0.99           |
   | CartPole    | 1500     | 0.004         | 0.99           |
   | MountainCar | 1500     | 0.0005        | 0.8            |
   | BipedalWalker| 15000   | 0.0001        | 0.99           |
   | CarRacing   | 200      | 0.0005        | 0.99           |

## ğŸ“Š Results

### Training Performance

| Environment | Success Criteria | Episodes to Solve | Final Performance |
|-------------|------------------|-------------------|-------------------|
| GridWorld   | Reach goal       | ~200              | âœ… Solved         |
| CartPole    | Avg reward â‰¥ 500 | ~700              | âœ… Solved         |
| MountainCar | Avg reward â‰¥ -100| ~200              | âœ… Solved         |
| BipedalWalker| Stable walking  | ~1000+            | ğŸ”„ Training       |
| CarRacing   | Complete track   | ~200+             | ğŸ”„ Training       |

### Learning Curves
The implementation includes visualization of training progress with reward plots showing:
- Episode vs. Average Reward
- Convergence patterns
- Performance stability

## ğŸš€ Installation

### Prerequisites
```bash
pip install torch torchvision
pip install gymnasium[all]
pip install numpy matplotlib
pip install pickle
pip install PIL
```

### Clone Repository
```bash
git clone <repository-url>
cd Actor-Critic-Algorithm
```

## ğŸ’» Usage

### Running the Complete Implementation
```python
# Open the Jupyter notebook
jupyter notebook hvenkatr_sugheerth_assignment3_part1_part2.ipynb
```

### Loading Trained Models
```python
import pickle
import torch

# Load trained models
with open('sugheert_hvenkatr_assignment3_cartpole_actor.pkl', 'rb') as f:
    actor_model = pickle.load(f)

with open('sugheert_hvenkatr_assignment3_cartpole_critic.pkl', 'rb') as f:
    critic_model = pickle.load(f)
```

### Training New Agent
```python
import gymnasium as gym

# Create environment
env = gym.make("CartPole-v1")

# Initialize agent
agent = A2C_Agent(env, max_reward=500, episodes=1500, 
                  discount=0.99, lr_actor=0.004, lr_critic=0.004)

# Train the agent
agent.train()

# Test the agent
agent.test()
```

## ğŸ“ File Structure

```
â”œâ”€â”€ hvenkatr_sugheerth_assignment3_part1_part2.ipynb    # Main implementation
â”œâ”€â”€ README.md                                            # This file
â”œâ”€â”€ LICENSE                                             # License file
â”‚
â”œâ”€â”€ Trained Models:
â”œâ”€â”€ sugheert_hvenkatr_assignment3_gridworld_actor.pkl   # GridWorld actor
â”œâ”€â”€ sugheert_hvenkatr_assignment3_gridworld_critic.pkl  # GridWorld critic
â”œâ”€â”€ sugheert_hvenkatr_assignment3_cartpole_actor.pkl    # CartPole actor
â”œâ”€â”€ sugheert_hvenkatr_assignment3_cartpole_critic.pkl   # CartPole critic
â”œâ”€â”€ sugheert_hvenkatr_assignment3_carracing_actor.pkl   # CarRacing actor
â”œâ”€â”€ sugheert_hvenkatr_assignment3_carracing_critic.pkl  # CarRacing critic
â”œâ”€â”€ sugheert_hvenkatr_assignment3_bipedal_walker_actor.pkl  # BipedalWalker actor
â”œâ”€â”€ sugheert_hvenkatr_assignment3_bipedal_walker_critic.pkl # BipedalWalker critic
â””â”€â”€ hvenkatr_sugheert_assignment3_part2_MountainCarContinuous (1).pkl # MountainCar weights
```

## âœ¨ Key Features

### ğŸ¯ Multi-Environment Support
- **Discrete Action Spaces**: GridWorld, CartPole
- **Continuous Action Spaces**: MountainCar, BipedalWalker, CarRacing
- **Image-based Observations**: CarRacing with CNN processing

### ğŸ§ª Advanced Techniques
- **Advantage Estimation**: Reduces variance in policy gradients
- **Separate Optimizers**: Independent learning rates for actor and critic
- **Experience Replay**: Efficient learning from collected trajectories
- **Reward Normalization**: Improved training stability

### ğŸ“ˆ Comprehensive Analysis
- **Training Visualization**: Real-time learning curves
- **Performance Metrics**: Episode rewards, convergence analysis
- **Model Persistence**: Save/load trained models
- **Testing Framework**: Evaluation on trained agents

### ğŸ”§ Modular Design
- **Reusable Components**: Base classes for different environment types
- **Configurable Hyperparameters**: Easy experimentation
- **Clean Architecture**: Separate concerns for different functionalities

## ğŸ“ Educational Value

This implementation serves as an excellent learning resource for:
- Understanding policy gradient methods
- Implementing actor-critic algorithms
- Working with different types of RL environments
- Handling both discrete and continuous action spaces
- Processing image-based observations with CNNs

---

**This project is for educational purposes as part of CSE 446/546 coursework.**